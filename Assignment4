Q1 Part A : 
    data lake on GCP Cloud Datastore
    
    import csv
    import sys
    import datetime

    from google.cloud import datastore
    client=datastore.Client()

    query=client.query(kind='movie')
    query_iter=query_fetch()

    for entity in query_iter():
	     print(entity)

   #### Output : Check Q1_Datastore.png
 ========================================================================================  
   
  Q4. 
  
  Write in Paraquet format
  
Specifying the Cloud Storage bucket
...................................
def get(self):
  bucket_name = os.environ.get('BUCKET_NAME',
                               app_identity.get_default_gcs_bucket_name())

  self.response.headers['Content-Type'] = 'text/plain'
  self.response.write('Demo GCS Application running from Version: '
                      + os.environ['CURRENT_VERSION_ID'] + '\n')
  self.response.write('Using bucket name: ' + bucket_name + '\n\n')
  
  Writing to Cloud Storage
  ........................
  def create_file(self, filename):
  """Create a file.

  The retry_params specified in the open call will override the default
  retry params for this particular file handle.

  Args:
    filename: filename.
  """
  self.response.write('Creating file %s\n' % filename)

  write_retry_params = gcs.RetryParams(backoff_factor=1.1)
  gcs_file = gcs.open(filename,
                      'w',
                      content_type='text/plain',
                      options={'x-goog-meta-foo': 'foo',
                               'x-goog-meta-bar': 'bar'},
                      retry_params=write_retry_params)
  gcs_file.write('abcde\n')
  gcs_file.write('f'*1024*4 + '\n')
  gcs_file.close()
  self.tmp_filenames_to_clean_up.append(filename)
  
Covert to Parequet
...................
from pyspark import SparkContext
from pyspark.sql import SQLContext
from pyspark.sql.types import *
import os

if __name__ == "__main__":
    sc = SparkContext(appName="CSV2Parquet")
    sqlContext = SQLContext(sc)

    schema = StructType([
            StructField("col1", IntegerType(), True),
            StructField("col2", StringType(), True),
            StructField("col3", StringType(), True),
            StructField("col4", StringType(), True)])
    dirname = os.path.dirname(os.path.abspath(__file__))
    csvfilename = os.path.join(dirname,'Temp.csv')    
    rdd = sc.textFile(csvfilename).map(lambda line: line.split(","))
    df = sqlContext.createDataFrame(rdd, schema)
    parquetfilename = os.path.join(dirname,'output.parquet')    
    df.write.mode('overwrite').parquet(parquetfilename)


Reading data
---------------
import sys
import csv
import datetime
import pandas as pd
import gcsfs

from google.cloud import storage

storage_client = storage.Client()
bucket = storage_client.get_bucket('dph_assignment_3')
blob = bucket.blob('movie_data.csv')
bt = blob.download_as_string()

from io import BytesIO
s = BytesIO(bt)
df=pd.read_csv(s)
print(df)

Q2 and 3
----------------

Submitting a job

  Procedure:
Create an external table in Hive pointing to your existing zipped CSV file.
Create another Hive table in parquet format
Insert overwrite parquet table with Hive table
Put all the above queries in a script and submit as a job.

Create a Job in the bucket with the query.
